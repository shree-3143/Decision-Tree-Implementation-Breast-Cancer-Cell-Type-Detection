# -*- coding: utf-8 -*-
"""BigO Efficiency Trials - Breast Cancer Implementation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TU9rkqd52QGsHz5Cl1xK7E9DXU-Rp5x-
"""

1#import libraries
import numpy as np #giving each of the libraries an alias, a variable they can be referenced as
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#load the data
from google.colab import files
uploaded = files.upload()
df = pd.read_csv("data.csv") #using the Pandas library to read the CSV, and this data is stored in the variable "df"

# return tuple that shows the dimensions of the dataset
df.shape

# axis=1 specifies columns
# drop all columns that contain any missing or null values
df = df.dropna(axis=1)

# see the new shape of the dataset after dropping the column
df.shape

# the class representing a decision node in the decision tree
# a decision node primarily contains information about a specific split
class Node():
  # initialises a new instance of a node
  # self is the specific instance of the class that is being manipulated
  # feature: feature used for splitting the node (defaults to None) - refers to a column label / property of a cell
  # threshold: threshold used for splitting the node (defaults to None)
  # left: the left child node (defaults to None)
  # right: the right child node (defaults to None)
  # gain: the information gain of the split (defaults to None)
  # value: if the node is a leaf node, this attribute holds the predicted value of the target variable (defaults to None)
  def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):
    self.feature = feature
    self.threshold = threshold
    self.left = left
    self.right = right
    self.gain = gain
    self.value = value

# decision tree classifier algorithm utilised for binary classification problems
class DecisionTree():
  # constructor for DecisionTree class - defines an instance of a Decision Tree
  # min_samples: an integer representing the minimum number of samples (datapoints) required to split an internal node - at least 2 samples are required to make 2 groups
  # max_depth: an integer representing the maximum depth of the decision tree - the tree is limited to a maximum of 2 levels of splits from the root node
  def __init__(self, min_samples=2, max_depth=2):
    self.min_samples = min_samples
    self.max_depth = max_depth


  # method for splitting the given dataset into 2 datasets, based on the given feature and threshold
  def split_data(self, dataset, feature, threshold):
    # dataset: input dataset in the form of ndarray
    # feature: int specifying the index of the feature to base the split on
    # threshold: float specifying the threshold value for the split

    # empty lists to store the left and right datasets after the split
    left_dataset = []
    right_dataset = []

    # loop through each row of the dataset array, and split each row based on the value of the feature in that row and the threshold
    for row in dataset:
      if row[feature] <= threshold:
        left_dataset.append(row)
      else:
        right_dataset.append(row)

    # convert lists containing left and right datasets after the split, into numpy arrays
    left_dataset = np.array(left_dataset)
    right_dataset = np.array(right_dataset)

    # return these arrays
    return left_dataset, right_dataset


  # method for calculating the entropy of the given label values (the target values we are aiming to classify each data point into)
  # input is an array of only target values (dependent dataset, provided in training)
  # method primarily used for the training data, to see the entropy of the target values (dependent training dataset) we are trying to match in our prediction using the independent training dataset
  def entropy(self, y):
    # y: input target values - the target values of the dependent training dataset we are trying to match
    # entropy: float that represents the entropy of the given target values

    entropy = 0

    # returns an array that contains the unique target values in y (the dependent dataset)
    # in this context, 1 or 0 (M or B classification of the cell)
    labels = np.unique(y)

    # loop over each of the unique target values (1 or 0)
    for label in labels:
      # filter out the array y to only obtain the elements that match the current label (target values)
      # create a new array containing only the instances of y that match the specified label
      label_examples = y[y == label]
      # calculate ratio of current label in y
      proportion = len(label_examples) / len(y)
      # calculate entropy in the dataset using the current label and ratio
      entropy += -proportion * np.log2(proportion)

    # return final calculated entropy value
    return entropy


  # method for calculating information gain (reduction in entropy) from ths split
  # analysis of corresponding y-values (dependent target values) for parent and child datasets
  def information_gain(self, parent, left, right):
    # parent: input parent dataset
    # left: left subset of parent dataset after split
    # right: right subset of parent dataset after split

    # set initial information gain to 0
    information_gain = 0

    # calculate entropy for parent dataset
    parent_entropy = self.entropy(parent)

    # calculate weight for left and right nodes
    # proportions are calculated as a ratio of the datapoints in the child dataset to the parent dataset
    weight_left = len(left) / len(parent)
    weight_right = len(right) / len(parent)

    # calculate entropy for left and right nodes
    entropy_left, entropy_right = self.entropy(left), self.entropy(right)

    # calculate weighted entropy
    # ensures that the contribution of each child node to the total entrop (entropy of the parent node) is proportional to the size of the node
    weighted_entropy = weight_left * entropy_left + weight_right * entropy_right

    # calculate information gain from split
    # allows us to see the reduction in entropy, after the split
    information_gain = parent_entropy - weighted_entropy

    #return calculated information gain
    return information_gain



  # method to find best split for the given dataset, that maximises the information gain from the split
  # function comprehensively searches for the best split, by comparing the information gain from each split by evaluating all features and thresholds
  def best_split(self, dataset, num_samples, num_features):
    # dataset: the array that contains the dataset that is to be split
    # num_samples: integer specifying the number of samples in the dataset (rows / patient data points)
    # num_features: integer specifying the number of features the dataset contains (column labels - in this context, properties of the cell)

    # dictionary to store best split values
    # default gain is -1, no split has veen found yet
    best_split = {
        "gain" : -1,
        "feature" : None,
        "threshold" : None,
    }

    for feature_index in range(num_features):
      # select the entire column for the current feature (for the current feature iteration)
      feature_values = dataset[:,feature_index]
      # thresholds: an array containing all the unique values from the values of the column given in as input
      thresholds = np.unique(feature_values)

      # loop over all unique values of the feature:
      # each of the unique values in the feature are considered as a threshold, and the information gain is evaluating for each split considering a specific threshold
      for threshold in thresholds:
        # get left and right datasets after the current split
        left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)

        # ensures that the subsets after the split aren't empty, and if they are, this threshold is skipped as an invalid split
        if len(left_dataset) and len(right_dataset):
          # obtain the y-values - the corresponding target values for each dataset (the part of the diagnosis column corresponding to each dataset - which will be the last column in the dataset)
          y, left_y, right_y = dataset[:,-1], left_dataset[:,-1], right_dataset[:,-1]

          # calculate information gain from the split
          information_gain = self.information_gain(y, left_y, right_y)

          # update the data for the best split, if the condition is met
          # if the information gain from this split using this specific threshold results in a higher information gain than any of the other thresholds considered before
          # update the data that has been utilised to maximise the information gain (put in the data from the specific split that allowed the higher information gain so far)
          if information_gain > best_split["gain"]:
            best_split["feature"] = feature_index
            best_split["threshold"] = threshold
            best_split["left_dataset"] = left_dataset
            best_split["right_dataset"] = right_dataset
            best_split["gain"] = information_gain # updated information gain (highest, from all the ones that have been tested so far in the loop)

    # return the dictionary that contains all the info regarding the feature and threshold that have been used to achieve the highest information gain, providing the best split
    # provides info for a single split, that is the most optimal, for the dataset provided in as input
    return best_split


  # method for calculating most occurring value in the given list of target values (y-values from the dependent dataset) for our dataset
  # y: array of y-values (target values we are trying to achieve, from the dependent dataset)
  def calculate_leaf_value(self, y):
    # array is converted into a Python list
    y = list(y)

    # select the element with the highest count in the list
    most_occurring_value = max(y, key=y.count)

    # return this value
    return most_occurring_value


  # method for recursively building a tree from the given dataset, after computing the best splits
  def build_tree(self, dataset, current_depth = 0):
    # dataset: the dataset to build the tree from
    # current_depth: int specifying the current depth of the tree

    # split the dataset into independent and dependent datasets
    X, y = dataset[:, :-1], dataset[:,-1]

    # n_samples: number of data point - patients (# of rows in dataset)
    # n_features: number of properties to be considered per cell (# of columns in dastaset)
    n_samples, n_features = X.shape

    # recursive splitting continues till the stopping conditions are met:
      # 1. the number of samples in the dataset is less than the minimum number of samples required for a split (2)
      # 2. the current depth of the tree is still less than the maximum depth allowed for the tree (2)
    if n_samples >= self.min_samples and current_depth <= self.max_depth:

      # get the best split
      best_split = self.best_split(dataset, n_samples, n_features)

      # check that the information gain isn't 0
      # otherwise, this isn't a meaningful split, and it jumps to the creation of a leaf node
      if best_split["gain"]:
        # function recursively calls to build left and right child nodes from current decision node, as long stopping criteria is not yet met
        # increment current depth after split (allows the child nodes to have an updated depth, without changing the current depth for the current decision node)
        left_node = self.build_tree(best_split["left_dataset"], current_depth + 1)
        right_node = self.build_tree(best_split["right_dataset"], current_depth + 1)

        # return the current decision node, with pieces of information and new child datasets from split
        return Node(best_split["feature"], best_split["threshold"], left_node, right_node, best_split["gain"])

    # once stopping criteria is met, form a leaf node containing the final groups of datapoints
    # the dataset this node pertains to classified as the majority value present in the dependent dataset that correlates to the datapoints (1 or 0)
    leaf_value = self.calculate_leaf_value(y)

    # return the leaf node value
    return Node(value=leaf_value)



  # function for building and fitting the tree to the given X and y datasets
  def fit(self, X, y):
    # X: the feature matrix (data about properties of cell, for each sample or patient)
    # y: the target values for each sample (the dependent dataset)

    # reshape y to have 2 dimensions (n_samples, 1)
    y = y.reshape(-1, 1)

    # combines the independent and dependent datasets into one dataset
    # diagnosis column (y) is added as the last column
    # axis=1 means the arrays are combined horizontally (along columns, not rows)
    dataset = np.concatenate((X,y), axis=1)

    # starting from the current dataset (root node), build the decision tree
    # root node holds the rest of the tree (result is stored in this root node)
    # using the concatenated dataset
    self.root = self.build_tree(dataset)



  # function for predicting the class labels (final classification) for each sample in the feature matrix X
  # X: the feature matrix, to make predictions for
  def predict(self, X):
    # empty list for storing the predictions
    predictions = []

    # for each sample (datapoint) in X, make a prediction by traversing the tree
    for x in X:
      # the make_prediction function returns the value of the leaf node each datapoint reaches in the tree
      prediction = self.make_prediction(x, self.root)

      # append the prediction to the list of predictions
      predictions.append(prediction)

    # convert the predictions list to a numpy array and return it
    np.array(predictions)

    return(predictions)


  # function for actually traversing the tree, to predict the target value for the given datapoint (a single patient's cell)
  def make_prediction(self, x, node):
    # x: single datapoint (patient's cell) for which we are predicting the target value
    # node: the current node being evaluated

    # if the current node is a leaf node, extract that leaf's node's final classification value and return it
    if node.value != None:
      # reutrn the datapoint's final classification value after it has reached a leaf node
      return node.value
    # otherwise, if the current node is NOT a leaf node, traverse through the tree accordingly
    else:
      feature = x[node.feature]
      # if the specific feature value of the datapoint is less than the threshold
      # utilising all the information from the nodes that have already been formed during the training of the tree
      if feature <= node.threshold:
        # recursively call make_prediction again on the left child node, as this is where the datapoint has landed
        # continue recursive call until data point reaches a leaf node
        return self.make_prediction(x, node.left)
      else:
        # recursively call make_prediction again on the right child node, as this is where the datapoint has landed
        return self.make_prediction(x, node.right)

# calculates the accuracy of the model
def accuracy(Y_test, Y_pred):
  # Y_test: array containing actual values from dependent dataset for testing
  # Y_pred: array containing the predicted label using the model

  # transform 2D array into standard 1D format, allowing for direct comparison to predicted values
  Y_test = Y_test.flatten()
  total_samples = len(Y_test)
  correct_predictions = np.sum(Y_test == Y_pred)
  return(correct_predictions/total_samples)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from timeit import default_timer as timer
import matplotlib.pyplot as plt
labelencoder_Y = LabelEncoder()
sc = StandardScaler()

# input sizes
input_sizes = [5,10,20,50,100,500]

# running_times will be stored for each input_size
running_times = []

# record running time for each input size in the list of input sizes
for size in input_sizes:
  # for the independent dataset (feature matrix), select all columns after the diagnosis column, and the number of rows selected each is the input size
  X = df.iloc[:size , 2:31].values
  # select the first (input size) number of rows in the diagnosis column
  # transform this categorical data into numerical data
  Y = labelencoder_Y.fit_transform(df.iloc[:size , 1].values)

  # split the datasets into training and testing datasets
  # The independent dataset (X) is split into 75% training and 25% testing
  # The dependent dataset (Y) is split into 75% training and 25% testing
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

  # scales the independent datasets (for both training and testing), so that they have a balanced scale
  X_train = sc.fit_transform(X_train)
  X_test = sc.fit_transform(X_test)

  # begin timer
  start = timer()

  # build decision tree using training data
  model = DecisionTree(2, 2)

  # fit the decision tree model to the training data
  model.fit(X_train, Y_train)

  # use the trained model (built decision tree) to make predictions on the test data
  predictions = model.predict(X_test)

  # end timer
  end = timer()

  # print accuracy
  print(f"Model's Accuracy for input size of {size}: {accuracy(Y_test, predictions)}")


  # calculate running time (measured in seconds)
  running_time = end - start

  # add it to our list of times
  running_times.append(running_time)

print("""
      Running times of inputs for Decision Tree Classifier:
""")
# print average "running times" for each input size
for i in range(len(input_sizes)):
 print("Running time for input size",input_sizes[i], "is", running_times[i])

# lists of our x and y vals (points are input_sizes[n], running_times[n])

# create the plot
plt.title("Growth in running time with inc. in input size")
plt.xlabel("Input Size (number of patients' cells being classified)")
plt.ylabel("Running Time (seconds)")
plt.plot(input_sizes, running_times, '-b', label='Decision Tree Classifier')
legend = plt.legend(loc='upper right')

# display the plot
plt.show()